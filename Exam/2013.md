# 2013 Exam
## Q1
1.  $w_i^{t-1} P_i^{t}(\mathbf{X}=x_t) = w_i^t$   
$v_i^{t+1}: ... : v_N^{t+1} = w_i^t: ... : w_N^t$   
$v_i^{t+1}: ... : v_N^{t+1} = w_i^{t}P_i^{t}(\mathbf{X}=x_t): ... : w_N^{t}P_N^{t}(\mathbf{X}=x_t)$   
$v_i^{t+1} = \frac{w_i^{t}P_i^{t}(x_t)}{\sum^N_{i=0} w_N^{t}P_N^{t}(x_t)}$   


2.  weight of tomorrow can be derived using weight of today.  
$v_i^{t+1}: ... : v_N^{t+1} = w_i^{t-1}P_i^{t}(\mathbf{X}=x_t): ... : w_N^{t-1}P_N^{t}(\mathbf{X}=x_t)$  
$v_i^{t+1}: ... : v_N^{t+1} = v_i^{t}P_i^{t}(\mathbf{X}=x_t): ... : v_N^{t}P_N^{t}(\mathbf{X}=x_t)$  
the 'algorithm' is simply multiplying yesterday weight with probability of being correct, and normalize $v^{t+1}$. Order of computation is $O(N)$ for each individual day and $O(TN)$ for all days (or if yesterday weight is not given).  

3. $Loss(x^T) = \sum^{T}_{t=1} -log(P^t(x_t))$  
$Loss(x^T) = \sum^{T}_{t=1} -log(\sum^N_{i=1}{v_i^tP_i^t(\mathbf{X})})$  
$Loss(x^T) = \sum^{T}_{t=1} -log(\sum^N_{i=1} \frac{\prod_{t'=1}^{t}{P_i^{t'}(\mathbf{X})}}{\sum_{i=1}^N \prod_{t'=1}^{t-1}{P_i^{t'}(\mathbf{X})}})$  
$Loss(x^T) = \sum^{T}_{t=1} -log( \frac{\sum^N_{i=1}\prod_{t'=1}^{t}{P_i^{t'}(\mathbf{X})}}{\sum_{i=1}^N \prod_{t'=1}^{t-1}{P_i^{t'}(\mathbf{X})}})$  
if we define $\prod_{t'=1}^{t}{P_i^{t'}(\mathbf{X})} = e^t$, the equation can be rewritten as  
$Loss(x^T) = \sum^{T}_{t=1} -log( \frac{\sum^N_{i=1}e^{t}}{\sum_{i=1}^N e^{t-1}})$   
$Loss(x^T) = -log( \frac{\sum^N_{i=1}e^{t}}{\sum_{i=1}^N e^{t-1}} * \frac{\sum^N_{i=1}e^{t-1}}{\sum_{i=1}^N e^{t-2}} *... * \frac{\sum^N_{i=1}e^{1}}{\sum_{i=1}^N e^{0}})$   
$Loss(x^T) = -log( \frac{\sum^N_{i=1}e^{t}}{N}) = -log( \sum^N_{i=1}e^{t}) + logN$
4. For predictor $i$, $Loss_i(x^T) = \sum^{T}_{t=1} -log(P^t_i(x_t))$.  
$Loss - Loss_i = -log( \frac{\sum^N_{i=1}e^{t}}{N}) = -log( \sum^N_{i=1}e^{t}) + logN - \sum^{T}_{t=1} -log(P^t_i(x_t))$   
$=-log( \sum^N_{i=1}e^{T}) + logN + log(e^T)$   
$=log(\frac{e^T}{ \sum^N_{i=1}e^{T}}) + logN$  
as $e^T < 1$, $log(\frac{e^T}{ \sum^N_{i=1}e^{T}}) < 0$.   
Therefore, $max (Loss - Loss_i) = logN$


## Q2
## 1. 
1. Miss ratio = 1, average memory access time = 1ms
2.   
Consider the program below, assume we only have 2 frames for buffer
|Algo|0|1|0|2|0|3|
|-|-|-|-|-|-|-|  
|FIFO - 0|__0__|0|0|__2__|2|__3__|  
|FIFO - 1|-|__1__|1|1|__0__|0|  
|LRU - 0|__0__|0|0|0|0|__3__|  
|LRU - 0|-|__1__|1|__2__|2|0|  
    Cache miss
    1. FIFO 5 times  
    2. LRU 4 times  
3. Second chance  
every page have a referenced bit, which is set every time the page is referenced, when a page with its referenced bit set is set to be removed from the main memory, instead of removing it, the referenced bit is erased, only when when it is attempted to be erased for a second time, with its referenced bit unset, it will be removed, essentially giving it a second chance.  

## Q3  
1. 60
2. 　　
(A) area　　
(B) centroid　　
$M_{00} = 12$　　
$M_{10}= 26$　　
3. I@k = 1, C@k=3   
4.  
    Centroid for C = (10, 27)  
    Centroid for I = (12, 25)  
    decision boundary = $M_{00} - M_{10} + 15$, if the equation return $>0$ for input it is classified as C.   
    Classification result for (13, 27) is C.  
5. kNN does not require a model building step, even for a large amount of data the preprocess (training) time, is zero. Building a decision boundary however, requires going through all data points before making prediction. On the other hand, kNN takes longer to make prediction, as it has to search for the k most nearest neighbours before making any decision. In contrast, decision boundary only need to feed the data into the equation and examine the result, which is faster.  


## Q4  
1. tf-idf (term frequency-inversed document frequency)    
    is a statistic use to determine the importance of a certrain word is to a document. As the name suggests, tf refers the frequency a certain term appears in a sentense or a single document (with some other variants, like term frequency, $f/\sum{f}$, at the same time, a word appear very often in a sentense can be rather useless for text mining, like the word "the". In order to offset this effect, inverse documment frequency is introduced, idf calculate the number of time a certain word appear in a large set of document, for example all wikipedia pages etc. 
 
2. ZMP (see other years)  
3. Distributed Hash